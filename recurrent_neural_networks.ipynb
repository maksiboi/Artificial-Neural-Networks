{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7437743,"sourceType":"datasetVersion","datasetId":4328849}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 7 Recurrent neural networks\nIn this exercise we will try a simple experiment with a recurrent neural network. One of the well-known recurrent neural network models is the so called Long short-term memory (LSTM) network. More information on LSTM can be found in the text [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n\n## 7.1 The MNIST dataset revisited (1)\nIn one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a recurrent neural network to the problem of digits classification. To keep it simple, we will use a simple LSTM network that will be fed with one row of the image at a time. With each new row, it will update its states and give its prediction. What we are interested in is its prediction after the last row i.e. after it has the full information.","metadata":{"id":"cphH-e6Mq3ji"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n# Constants\nlearning_rate = 0.001\nnum_epochs = 10\nbatch_size = 100\n#we will feed a row at a time to the LSTM and there are 28 rows per image\ntimesteps = 28\n#each row has 28 columns whose values are simultaneously passed to LSTM\nn_input = 28 # MNIST data input (img shape: 28*28)\n#the number of hidden states in the LSTM\nn_hidden = 128\nn_classes = 10\n\n# Data transformation\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\ntrain_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataset = datasets.MNIST(root='./data', train=False, transform=transform)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Define the LSTM model\nclass LSTMModel(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTMModel, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n        out, _ = self.lstm(x, (h0, c0))\n        out = self.fc(out[:, -1, :])\n        return out\n\n# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = LSTMModel(n_input, n_hidden, 1, n_classes).to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\ntimesteps_values = list(range(1, timesteps))\naccuracy_values = []\n\n\n# Training loop\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.view(-1, timesteps, n_input).to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    # Test the model\n    if (epoch + 1) % 1 == 0:\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                images = images.view(-1, timesteps, n_input).to(device)\n                labels = labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')","metadata":{"id":"h69cwQIWq3jm","execution":{"iopub.status.busy":"2024-01-22T17:11:25.399982Z","iopub.execute_input":"2024-01-22T17:11:25.400877Z","iopub.status.idle":"2024-01-22T17:13:54.337007Z","shell.execute_reply.started":"2024-01-22T17:11:25.400845Z","shell.execute_reply":"2024-01-22T17:13:54.336016Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stdout","text":"Epoch [1/10], Test Accuracy: 96.67%\nEpoch [2/10], Test Accuracy: 97.82%\nEpoch [3/10], Test Accuracy: 98.04%\nEpoch [4/10], Test Accuracy: 98.11%\nEpoch [5/10], Test Accuracy: 98.35%\nEpoch [6/10], Test Accuracy: 98.43%\nEpoch [7/10], Test Accuracy: 98.62%\nEpoch [8/10], Test Accuracy: 98.51%\nEpoch [9/10], Test Accuracy: 98.71%\nEpoch [10/10], Test Accuracy: 98.88%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Tasks**\n\n1. Study and run the code above.\n2. Draw a plot that shows the relation between the number of rows given to the network and its final accuracy on the test set.\n3. What happens if we use gradient descent instead of Adam?\n","metadata":{"id":"iRbpYMdrq3jn"}},{"cell_type":"code","source":"timesteps_values = list(range(1, timesteps))\naccuracy_values = []\n\nfor timesteps_value in timesteps_values: \n    # Training loop\n        for i, (images, labels) in enumerate(train_loader):\n            images = images.view(-1, timesteps, n_input)[:, :timesteps_value, :].to(device)\n            labels = labels.to(device)\n\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            # Backward pass and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Test the model\n        if (timesteps_value + 1) % 1 == 0:\n            with torch.no_grad():\n                correct = 0\n                total = 0\n                for images, labels in test_loader:\n                    images = images.view(-1, timesteps, n_input)[:, :timesteps_value, :].to(device)\n                    labels = labels.to(device)\n                    outputs = model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n\n                accuracy = 100 * correct / total\n                accuracy_values.append(accuracy)\n                print(f'Timesteps [{timesteps_value}], Test Accuracy: {accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:23:28.072347Z","iopub.execute_input":"2024-01-22T17:23:28.072748Z","iopub.status.idle":"2024-01-22T17:30:29.427295Z","shell.execute_reply.started":"2024-01-22T17:23:28.072715Z","shell.execute_reply":"2024-01-22T17:30:29.426206Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Timesteps [1], Test Accuracy: 11.35%\nTimesteps [2], Test Accuracy: 11.78%\nTimesteps [3], Test Accuracy: 15.27%\nTimesteps [4], Test Accuracy: 22.06%\nTimesteps [5], Test Accuracy: 34.56%\nTimesteps [6], Test Accuracy: 50.44%\nTimesteps [7], Test Accuracy: 62.38%\nTimesteps [8], Test Accuracy: 68.72%\nTimesteps [9], Test Accuracy: 75.63%\nTimesteps [10], Test Accuracy: 80.22%\nTimesteps [11], Test Accuracy: 85.12%\nTimesteps [12], Test Accuracy: 89.34%\nTimesteps [13], Test Accuracy: 92.25%\nTimesteps [14], Test Accuracy: 93.74%\nTimesteps [15], Test Accuracy: 94.56%\nTimesteps [16], Test Accuracy: 95.55%\nTimesteps [17], Test Accuracy: 96.57%\nTimesteps [18], Test Accuracy: 96.98%\nTimesteps [19], Test Accuracy: 97.39%\nTimesteps [20], Test Accuracy: 97.53%\nTimesteps [21], Test Accuracy: 97.48%\nTimesteps [22], Test Accuracy: 97.54%\nTimesteps [23], Test Accuracy: 97.40%\nTimesteps [24], Test Accuracy: 97.57%\nTimesteps [25], Test Accuracy: 97.66%\nTimesteps [26], Test Accuracy: 97.89%\nTimesteps [27], Test Accuracy: 97.65%\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n# Plotting\nplt.plot(timesteps_values, accuracy_values, marker='o')\nplt.title('Accuracy vs. Number of Rows')\nplt.xlabel('Number of Rows')\nplt.ylabel('Test Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:31:08.317093Z","iopub.execute_input":"2024-01-22T17:31:08.317472Z","iopub.status.idle":"2024-01-22T17:31:08.583814Z","shell.execute_reply.started":"2024-01-22T17:31:08.317439Z","shell.execute_reply":"2024-01-22T17:31:08.582922Z"},"trusted":true},"execution_count":60,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"optimizer_gradient_descent = optim.SGD(model.parameters(), lr=learning_rate)\n\naccuracies_w_gradient_descent = []\n# Training loop\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        images = images.view(-1, timesteps, n_input).to(device)\n        labels = labels.to(device)\n\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n\n        # Backward pass and optimize\n        optimizer_gradient_descent.zero_grad()\n        loss.backward()\n        optimizer_gradient_descent.step()\n\n    # Test the model\n    if (epoch + 1) % 1 == 0:\n        with torch.no_grad():\n            correct = 0\n            total = 0\n            for images, labels in test_loader:\n                images = images.view(-1, timesteps, n_input).to(device)\n                labels = labels.to(device)\n                outputs = model(images)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n\n            accuracy = 100 * correct / total\n            accuracies_w_gradient_descent.append(accuracy)\n            print(f'Epoch [{epoch + 1}/{num_epochs}], Test Accuracy: {accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:31:20.031771Z","iopub.execute_input":"2024-01-22T17:31:20.032890Z","iopub.status.idle":"2024-01-22T17:33:50.685048Z","shell.execute_reply.started":"2024-01-22T17:31:20.032855Z","shell.execute_reply":"2024-01-22T17:33:50.684113Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Epoch [1/10], Test Accuracy: 97.73%\nEpoch [2/10], Test Accuracy: 97.80%\nEpoch [3/10], Test Accuracy: 97.83%\nEpoch [4/10], Test Accuracy: 97.87%\nEpoch [5/10], Test Accuracy: 97.93%\nEpoch [6/10], Test Accuracy: 97.94%\nEpoch [7/10], Test Accuracy: 97.97%\nEpoch [8/10], Test Accuracy: 97.98%\nEpoch [9/10], Test Accuracy: 97.99%\nEpoch [10/10], Test Accuracy: 98.05%\n","output_type":"stream"}]},{"cell_type":"code","source":"# Corresponding number of rows (timesteps)\nnum_rows_values = list(range(1, num_epochs + 1))\n\n# Plotting\nplt.plot(num_rows_values, accuracies_w_gradient_descent, marker='o')\nplt.title('Accuracy using SGD as optimizer')\nplt.xlabel('Epoch')\nplt.ylabel('Test Accuracy')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-22T17:34:33.291141Z","iopub.execute_input":"2024-01-22T17:34:33.292058Z","iopub.status.idle":"2024-01-22T17:34:33.567344Z","shell.execute_reply.started":"2024-01-22T17:34:33.292022Z","shell.execute_reply":"2024-01-22T17:34:33.566490Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"If we use gradient descent instead of Adam as our optimization algorithm, we can observe that SGD achieves a consistently high test accuracy with a steady increase over epochs, also accuracy remains relatively stable, with a slight improvement in each epoch when using SGD as optimizer.\n\n**In conclusion, SGD appears to converge slightly faster and more steadily in my case.**","metadata":{}},{"cell_type":"markdown","source":"# 8 Convolutional neural networks\n\n\nThe goal of this exercise is to learn the basic stuff about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN or ConvNet). In the previous exercises, the building blocks mostly included simple operations that had some kind of activations functions and each layer was usually fully connected to the previous one. CNNs take into account the spatial nature of the input data, e.g. an image, and they process it by applying one or more  [kernels](https://en.wikipedia.org/wiki/Kernel_%28image_processing%29). In the case of images, this processing i.e. convolving is also known as filtering. The results of processing the input with a single kernel will be a single channel, but usually a convolutional layer involves more kernels producing more channels. These channels are often called **feature maps** because each kernel is specialized for extraction of a certain kind of features from the input. These feature maps are then combined into a single tensor that can be viewed as an image with multiple channels that can be then passed to further convolutional layers.\n\nFor example, if the input consists of a grayscale image i.e. an image with only one channel and a $5\\times 5$ kernel is applied, the result is a single feature map. The borders of the input image are usually padded with zeros to ensure that the resulting feature maps has the same number of rows and columns as the input image.\n\nIf the input consists of a color image i.e. an image with three channels and a $5\\times 5$ kernel is applied, what will actually be applied is an $5\\times 5\\times 3$ kernel that will simultaneously process all three channels and the result will again be a single feature map. However, if e.g. 16 several kernels are applied, then the result will be 16 feature maps. Should they be passed to another convolutional layer, **each** of its kernels would simultaneously process **all** feature maps so their sizes would be e.g. $3\\times 3\\times 16$ or $5\\times 5\\times 16$ where 16 is used to reach all feature maps simultaneously.\n\nThe convolution is usually followed by applying an element-wise non-linear operation to each of the values in the feature maps. Finally, what often follows is the summarization i.e. pooling of the information in the feature maps to reduce the spatial dimensions and keep only the most important information. A common approach used here is the so called max pooling. It is a non-linear downsampling where the input is divided into a set of non-overlapping rectangles and for each of them only the maximum value inside of it is kept.\n\n![Model of a neuron](data:image/png;base64,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)\n<center>Figure 1. Max pooling with $2\\times 2$ rectangles (taken from [Wikipedia](https://en.wikipedia.org/wiki/File:Max_pooling.png)).</center>\n\nWhat usually follows after several convolutional layers is putting the values of all feature maps into a single vector, which is then passed further to fully connected or other kinds of layers.\n\nThe number of parameters in the convolutional depends on the number of feature maps and the sizes of the kernels. For example, if a convolutional layer with 32 kernels of nominal size $3\\times 3$ receives 16 feature maps on its input, it will require $16\\times 3\\times 3\\times 32+32$ where the last 32 parameters refer to the kernel biases.","metadata":{"id":"NNpp9Ppweg3Q"}},{"cell_type":"markdown","source":"## 8.1 The MNIST dataset revisited (2)\nIn one of the previous exercises the MNIST dataset was used to demonstrate the use of multilayer perceptron. Here we are going to apply a convolutional neural network to the digit classification problem. We will use the following layers to build our model:\n\n* [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)\n* [torch.nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n* [torch.nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)\n* [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n\nThe [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) layer has the same effect as the fully connected layer, a matrix multiplication that was used in the previous exercise with the MNIST dataset.\n\n**Tasks**\n\n1. Study and run the code below. How is the accuracy compared to the ones obtained in the previous exerises with MNIST?\n2. Try to change the number and size of convolutional and fully connected layers. What has the greatest impact on the accuracy? For each network architecture configuration calculate the number of trainable parameters.\n3. What happens to the accuracy if another [non-linearity](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity) is used instead of ReLU? Experiment with at least two different activation functions.","metadata":{"id":"OJdpRk2gV9q4"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport tqdm","metadata":{"id":"m_1TWwNif8WE","execution":{"iopub.status.busy":"2024-01-21T15:48:56.017732Z","iopub.execute_input":"2024-01-21T15:48:56.018115Z","iopub.status.idle":"2024-01-21T15:48:56.022987Z","shell.execute_reply.started":"2024-01-21T15:48:56.018085Z","shell.execute_reply":"2024-01-21T15:48:56.021920Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu1 = nn.ReLU()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.relu2 = nn.ReLU()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.relu3 = nn.ReLU()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.relu1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.relu2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.relu3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits\n","metadata":{"id":"NK79DP85edIb","execution":{"iopub.status.busy":"2024-01-21T15:49:17.820606Z","iopub.execute_input":"2024-01-21T15:49:17.821420Z","iopub.status.idle":"2024-01-21T15:49:17.831910Z","shell.execute_reply.started":"2024-01-21T15:49:17.821386Z","shell.execute_reply":"2024-01-21T15:49:17.830903Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, device, train_dataloader, optimizer, epoch):\n\n    model.train()\n\n    train_loss = 0.0\n\n    for batch in tqdm.tqdm(train_dataloader):\n\n      # Every data instance is an input image + label pair\n      images, labels = batch\n\n      # It is necessary to have both the model, and the data on the same device, either CPU or GPU, for the model to process data.\n      # Data on CPU and model on GPU, or vice-versa, will result in a Runtime error.\n      images, labels = images.to(device), labels.to(device)\n\n      # Zero your gradients for every batch\n      optimizer.zero_grad()\n\n      # Make predictions for this batch\n      pred_logits = model(images)\n\n      # Compute the loss\n      loss = loss_fn(pred_logits, labels)\n\n      # Calculates the backward gradients over the learning weights\n      loss.backward()\n\n      # Tells the optimizer to perform one learning step\n      # Adjust the model�s learning weights based on the observed gradients for this batch\n      optimizer.step()\n\n      train_loss += loss.item()\n\n    # Print epoch's average loss\n    print(\"Epoch {} - Training loss: {}\".format(epoch+1, train_loss/len(train_dataloader)))\n\n\ndef evaluation(model, device, test_dataloader, epoch):\n\n    # Sets layers like dropout and batch normalization to evaluation mode before running inference\n    # Failing to do this will yield inconsistent inference results\n    model.eval()\n\n    test_accuracy = 0.0\n\n    # Disabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward().\n    # It will reduce memory consumption for computations that would otherwise have requires_grad=True.\n    with torch.no_grad():\n\n      for batch in tqdm.tqdm(test_dataloader):\n\n        images, labels = batch\n\n        images, labels = images.to(device), labels.to(device)\n\n        pred_logits = model(images)\n\n        probabilities = torch.nn.functional.softmax(pred_logits, dim=1)\n\n        # Find the index of the highest probability\n        predictions = probabilities.argmax(dim=1)\n\n        # Caluculate average batch accuracy\n        batch_accuracy = torch.mean((predictions == labels).float())\n\n        test_accuracy += batch_accuracy\n\n      print(\"Epoch {} - Accuracy: {}\".format(epoch+1, test_accuracy/len(test_dataloader)))\n\n\n","metadata":{"id":"FFJtOH6oNfLR","execution":{"iopub.status.busy":"2024-01-21T15:49:22.451872Z","iopub.execute_input":"2024-01-21T15:49:22.452226Z","iopub.status.idle":"2024-01-21T15:49:22.462996Z","shell.execute_reply.started":"2024-01-21T15:49:22.452192Z","shell.execute_reply":"2024-01-21T15:49:22.461946Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Move model to GPU if possible\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nmodel.to(device)\n\n# Augmentations\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))\n    ])\n\n# Stores the samples and their corresponding labels\ntrain_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\ntest_dataset = datasets.MNIST('../data', train=False, transform=transform)\n\n# Wraps an iterable around the Dataset to enable easy access to the samples.\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"xpUS9zIEoZ4p","execution":{"iopub.status.busy":"2024-01-21T15:49:27.412421Z","iopub.execute_input":"2024-01-21T15:49:27.413059Z","iopub.status.idle":"2024-01-21T15:49:30.406365Z","shell.execute_reply.started":"2024-01-21T15:49:27.413026Z","shell.execute_reply":"2024-01-21T15:49:30.405447Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 9912422/9912422 [00:00<00:00, 316177126.46it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 28881/28881 [00:00<00:00, 99700159.53it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1648877/1648877 [00:00<00:00, 147353547.46it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","output_type":"stream"},{"name":"stderr","text":"\n100%|██████████| 4542/4542 [00:00<00:00, 7753572.96it/s]","output_type":"stream"},{"name":"stdout","text":"Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)\n\n","metadata":{"id":"dPXNNzNl7Lzb","execution":{"iopub.status.busy":"2024-01-21T15:49:43.285252Z","iopub.execute_input":"2024-01-21T15:49:43.286085Z","iopub.status.idle":"2024-01-21T15:51:04.382085Z","shell.execute_reply.started":"2024-01-21T15:49:43.286051Z","shell.execute_reply":"2024-01-21T15:51:04.381207Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.11488066453801672\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.984375\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 66.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.038377363448414005\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9871616363525391\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 66.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.02699944500303617\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9880573153495789\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 66.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.020985970914107636\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 78.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9908439517021179\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 66.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.015905079765705595\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.00it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.990346372127533\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"- LSTM with Adam optimizer: Test Accuracy 98.22%\n- LSTM with SGD optimizer: Test Accuracy 98.90%\n- CNN: Test Accuracy 99.03%\n\n**In summary, the CNN outperformed both LSTM models in terms of accuracy on the MNIST dataset.**\n","metadata":{}},{"cell_type":"code","source":"#function for calculating the number of trainable parameters in the model.\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:06:17.400509Z","iopub.execute_input":"2024-01-21T16:06:17.401213Z","iopub.status.idle":"2024-01-21T16:06:17.406085Z","shell.execute_reply.started":"2024-01-21T16:06:17.401174Z","shell.execute_reply":"2024-01-21T16:06:17.404990Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Define the ranges for grid search\nchannel_1_values = [32, 64]\nchannel_2_values = [64, 128]\nfully_connected_values = [128, 256]\n\n# Dictionary to store models\nmodels = {}\n\n# Index counter for naming each architecture\nindex_counter = 1\n\n# Perform grid search\nfor n_channels_1 in channel_1_values:\n    for n_channels_2 in channel_2_values:\n        for n_fully_connected in fully_connected_values:\n            # Create and train the model with current hyperparameters\n            model = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size=5)  \n            model.to(device)\n\n            # Define loss function\n            loss_fn = nn.CrossEntropyLoss()\n\n            # Define optimizer\n            optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n            # Training loop\n            for epoch in range(training_epochs_count):\n                train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n                if (epoch + 1) % display_step == 0:\n                    evaluation(model, device, test_dataloader, epoch)\n\n            # Save the model in the dictionary\n            model_key = f\"{index_counter}_architecture\"\n            models[model_key] = model\n\n            # Increment index counter\n            index_counter += 1\n\n            # Print current hyperparameters and accuracy\n            print(f\"Model {model_key}: Hyperparameters - n_channels_1={n_channels_1}, n_channels_2={n_channels_2}, \"\n                  f\"n_fully_connected={n_fully_connected}, kernel_size=5\")","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:14:33.334550Z","iopub.execute_input":"2024-01-21T16:14:33.335493Z","iopub.status.idle":"2024-01-21T16:25:49.543121Z","shell.execute_reply.started":"2024-01-21T16:14:33.335458Z","shell.execute_reply":"2024-01-21T16:25:49.542173Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 66.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.12084598535126242\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9850716590881348\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.0401513973584345\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.987460196018219\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.028046231503595722\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9906449317932129\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01938518175494677\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.990744411945343\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.01579659975716479\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9889530539512634\nModel 1_architecture: Hyperparameters - n_channels_1=32, n_channels_2=64, n_fully_connected=128, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.11117338110890879\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.984375\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03788297840092244\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9862659573554993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.024626382854075386\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 70.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9912420511245728\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.019622291188044445\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9896497130393982\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 65.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.01584575427007666\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9918391704559326\nModel 2_architecture: Hyperparameters - n_channels_1=32, n_channels_2=64, n_fully_connected=256, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 62.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.1092052718043389\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9884554147720337\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 62.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03515351919838583\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9858678579330444\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.025886869412161277\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.990047812461853\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 62.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01796417182580052\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.70it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9919387102127075\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 62.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.014354452799924243\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9911425113677979\nModel 3_architecture: Hyperparameters - n_channels_1=32, n_channels_2=128, n_fully_connected=128, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.11333822122830123\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9879578351974487\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.038347938616917945\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9896497130393982\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 62.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.024145733172640276\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 71.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9926353693008423\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.017903799297462233\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9901472926139832\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 62.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.01445985885929947\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.990744411945343\nModel 4_architecture: Hyperparameters - n_channels_1=32, n_channels_2=128, n_fully_connected=256, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.11429939881173908\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9839769005775452\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.93it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.038847468089082585\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9911425113677979\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.025538947853222013\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9896497130393982\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01986459126135602\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:01<00:00, 79.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9908439517021179\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.014651372971474475\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9918391704559326\nModel 5_architecture: Hyperparameters - n_channels_1=64, n_channels_2=64, n_fully_connected=128, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.10767046618821428\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9862659573554993\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03705776339879747\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9894506335258484\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.02560858254846588\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9892516136169434\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 63.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01816290924766797\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9892516136169434\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:14<00:00, 64.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.015480027042598462\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.79it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9883559346199036\nModel 6_architecture: Hyperparameters - n_channels_1=64, n_channels_2=64, n_fully_connected=256, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.11436843024944064\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9799960255622864\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03860349488988663\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9873606562614441\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.42it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.025047687183695298\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9887539744377136\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.01791688322791083\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9893511533737183\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.39it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.015302547575183411\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9885549545288086\nModel 7_architecture: Hyperparameters - n_channels_1=64, n_channels_2=128, n_fully_connected=128, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.12100700861022277\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 73.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9867635369300842\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.03978660490526669\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9880573153495789\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.026067869463709117\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9914410710334778\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.02024858795925817\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9902468323707581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.014971344256439192\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 77.11it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9911425113677979\nModel 8_architecture: Hyperparameters - n_channels_1=64, n_channels_2=128, n_fully_connected=256, kernel_size=5\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Based on the outputs of my grid search, it seems that changing the number of fully connected neurons (n_fully_connected) has a significant impact on the accuracy.\n\nLooking just models, **Model 2_architecture** has the highest accuracy (99.184%) among the tested configurations. ","metadata":{}},{"cell_type":"code","source":"for model_key, model in models.items():\n    num_parameters = count_parameters(model)\n    print(f\"Model {model_key}: Number of Trainable Parameters - {num_parameters}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:31:19.862402Z","iopub.execute_input":"2024-01-21T16:31:19.863099Z","iopub.status.idle":"2024-01-21T16:31:19.869037Z","shell.execute_reply.started":"2024-01-21T16:31:19.863062Z","shell.execute_reply":"2024-01-21T16:31:19.868138Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Model 1_architecture: Number of Trainable Parameters - 454922\nModel 2_architecture: Number of Trainable Parameters - 857738\nModel 3_architecture: Number of Trainable Parameters - 907594\nModel 4_architecture: Number of Trainable Parameters - 1711818\nModel 5_architecture: Number of Trainable Parameters - 506954\nModel 6_architecture: Number of Trainable Parameters - 909770\nModel 7_architecture: Number of Trainable Parameters - 1010826\nModel 8_architecture: Number of Trainable Parameters - 1815050\n","output_type":"stream"}]},{"cell_type":"code","source":"#using \nclass CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.sigmoid1 = nn.Sigmoid()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.sigmoid2 = nn.Sigmoid()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.sigmoid3 = nn.Sigmoid()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.sigmoid1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.sigmoid2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.sigmoid3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:39:00.733547Z","iopub.execute_input":"2024-01-21T16:39:00.733934Z","iopub.status.idle":"2024-01-21T16:39:00.745356Z","shell.execute_reply.started":"2024-01-21T16:39:00.733905Z","shell.execute_reply":"2024-01-21T16:39:00.744313Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel_sigmoid = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:39:03.954043Z","iopub.execute_input":"2024-01-21T16:39:03.954407Z","iopub.status.idle":"2024-01-21T16:40:32.762419Z","shell.execute_reply.started":"2024-01-21T16:39:03.954374Z","shell.execute_reply":"2024-01-21T16:40:32.761476Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.01678839928613265\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 72.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9881568551063538\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.01003684597267849\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.9885549545288086\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.01034647139634048\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9876592755317688\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.008925894190710376\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9924363493919373\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.41it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.007587560464789254\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.23it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9912420511245728\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this specific case, using **Sigmoid** seems to yield slightly higher accuracy during some epochs compared to ReLU. \n\nFinal accurary when using Sigmoid is **slightly larger** then using ReLu","metadata":{}},{"cell_type":"code","source":"#using \nclass CNN(nn.Module):\n\n  # Method that defines the layers and other components of a model\n  def __init__(self,\n               input_channels,\n               n_channels_1,\n               n_channels_2,\n               n_fully_connected,\n               n_classes,\n               kernel_size\n               ):\n\n    super(CNN, self).__init__()\n\n    self.conv1 = nn.Conv2d(in_channels=input_channels,\n                           out_channels=n_channels_1,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.Hardtanh1 = nn.Hardtanh()\n\n    self.maxpool1 = nn.MaxPool2d((2,2))\n\n    self.conv2 = nn.Conv2d(in_channels=n_channels_1,\n                           out_channels=n_channels_2,\n                           kernel_size=kernel_size,\n                           padding='same'\n                           )\n\n    self.Hardtanh2 = nn.Hardtanh()\n\n    self.maxpool2 = nn.MaxPool2d((2,2))\n\n    self.fc1 = nn.Linear(in_features=7*7*n_channels_2, out_features=n_fully_connected, bias=True)\n\n    self.Hardtanh3 = nn.Hardtanh()\n\n    self.fc2 = nn.Linear(in_features=n_fully_connected, out_features=n_classes, bias=True)\n\n  # Method where the computation gets done\n  def forward(self, x):\n\n    # First convolutional layer\n    # We will apply n_channels_1 kernels of size kernel_size X kernel_size\n    # We are padding the input in order for the result to have the same number of rows and columns\n    x = self.conv1(x)\n\n    # Applying the non-linearity\n    x = self.Hardtanh1(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool1(x)\n\n    # Second convolutional layer\n    # We will apply n_channels_2 kernels of size kernel_size X kernel_size\n    x = self.conv2(x)\n\n    # again, we apply the non-linearity\n    x = self.Hardtanh2(x)\n\n    # and max pooling again, now each feature map will be of size 7 X 7\n    x = self.maxpool2(x)\n\n    # Flatten all dimensions except the batch\n    x = torch.flatten(x, 1)\n\n    # Fully connected layer\n    x = self.fc1(x)\n\n    # and again, we apply the non-linearity\n    x = self.Hardtanh3(x)\n\n    # Non-linearity\n    pred_logits = self.fc2(x)\n\n    return pred_logits","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:44:39.728448Z","iopub.execute_input":"2024-01-21T16:44:39.729219Z","iopub.status.idle":"2024-01-21T16:44:39.739930Z","shell.execute_reply.started":"2024-01-21T16:44:39.729185Z","shell.execute_reply":"2024-01-21T16:44:39.738927Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Architecture configs\ninput_channels=1\nn_channels_1=32\nn_channels_2=64\nn_classes=10\nn_fully_connected=128\nkernel_size=5\n\n# Training configs\ntraining_epochs_count = 5\nbatch_size = 64\nlearning_rate = 0.001\ndisplay_step=1\n\n# Model\nmodel_hard_than = CNN(input_channels, n_channels_1, n_channels_2, n_fully_connected, n_classes, kernel_size)\n\n# Define loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training\nfor epoch in range(training_epochs_count):\n\n  train_epoch(model, device, train_dataloader, optimizer, epoch)\n\n  if (epoch + 1) % display_step == 0:\n\n    evaluation(model, device, test_dataloader, epoch)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T16:45:02.160976Z","iopub.execute_input":"2024-01-21T16:45:02.161656Z","iopub.status.idle":"2024-01-21T16:46:30.094155Z","shell.execute_reply.started":"2024-01-21T16:45:02.161625Z","shell.execute_reply":"2024-01-21T16:46:30.093275Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Training loss: 0.009652400466326596\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Accuracy: 0.9876592755317688\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.38it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Training loss: 0.006328557174172927\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Accuracy: 0.990545392036438\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 60.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Training loss: 0.007455881687330826\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 76.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Accuracy: 0.9896497130393982\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 59.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Training loss: 0.0037747270913363017\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 74.72it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Accuracy: 0.9902468323707581\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 938/938 [00:15<00:00, 61.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Training loss: 0.007510349024403278\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 157/157 [00:02<00:00, 75.01it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Accuracy: 0.9902468323707581\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"In this specific case, using **HardTanh** seems to yield similar or slightly higher accuracy during some epochs compared to ReLU.\n\nFinal accuracy when using HardThan is **slightly smaller** when using ReLu","metadata":{}},{"cell_type":"markdown","source":"## 8.2 Image classification\nImage classification is a challenging computer vision problem with the best-known competition being [The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)](http://www.image-net.org/challenges/LSVRC/), which includes the ImageNet dataset with millions of $224\\times 224$ training images. The class names in one of the tasks there can be found [here](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). One of the most important breakthroughs was when in 2012 the convolutional neural network [AlexNet](https://en.wikipedia.org/wiki/AlexNet) won the first place. Ever since many highly successful convolutional neural networks architectures have been proposed, e.g. [VGG-16](https://arxiv.org/abs/1409.1556), [VGG-19](https://arxiv.org/abs/1409.1556), [ResNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf), [Inception](https://arxiv.org/abs/1409.4842), etc. Training such networks requires a lot of time because they have many layers with millions of parameters. In this exercise we are going to experiment with pre-trained models of some of the best known architectures.\n\n### 8.2.1 Using pre-trained models\nTry running the following code:","metadata":{"id":"EsU9MOc8QmDe"}},{"cell_type":"code","source":"import torchvision.models as models\nfrom torchvision.io import read_image\n\n# List of architectures to compare\narchitectures = [\"resnet34\", \"vgg16\", \"vgg19\", \"inceptionv3\"]\nimage_paths=[\"../input/cnn-img/cnn_img/badger.jpg\", \"../input/cnn-img/cnn_img/rabbit.jpg\", \"../input/cnn-img/cnn_img/sundial.jpg\", \"../input/cnn-img/cnn_img/pineapple.jpg\", \"../input/cnn-img/cnn_img/can.jpg\"]\n\n# Dictionary to store results\nresults = {}\n\n# Iterate over architectures\nfor architecture in architectures:\n    if architecture == \"resnet34\":\n        weights = models.ResNet34_Weights.DEFAULT\n        model = models.resnet34(weights=weights)\n    elif architecture == \"vgg16\":\n        weights = models.VGG16_Weights.DEFAULT\n        model = models.vgg16(pretrained=weights)\n    elif architecture == \"vgg19\":\n        weights = models.VGG19_Weights.DEFAULT\n        model = models.vgg19(pretrained=weights)\n    elif architecture == \"inceptionv3\":\n        weights = models.Inception_V3_Weights.DEFAULT\n        model = models.inception_v3(pretrained=weights)\n\n    model.eval()\n\n    # List to store results for each image\n    image_results = []\n\n    for path in image_paths:\n        # loading the image and rescaling it to fit the size for the imagenet architectures\n        img = read_image(path)\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n\n        prediction = model(batch).squeeze(0).softmax(0)\n        class_id = prediction.argmax().item()\n        score = prediction[class_id].item()\n        category_name = weights.meta[\"categories\"][class_id]\n\n        image_results.append({\"category\": category_name, \"score\": score})\n\n    results[architecture] = image_results","metadata":{"id":"qOJamRVpQrdo","execution":{"iopub.status.busy":"2024-01-22T15:22:50.541065Z","iopub.execute_input":"2024-01-22T15:22:50.541433Z","iopub.status.idle":"2024-01-22T15:22:57.380702Z","shell.execute_reply.started":"2024-01-22T15:22:50.541405Z","shell.execute_reply":"2024-01-22T15:22:57.379857Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"for architecture, image_results in results.items():\n    print(f\"\\nResults for {architecture} architecture:\")\n    for idx, result in enumerate(image_results, start=1):\n        print(f\"Image {idx}: {result['category']} - {100 * result['score']:.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:23:42.420774Z","iopub.execute_input":"2024-01-22T15:23:42.421239Z","iopub.status.idle":"2024-01-22T15:23:42.429107Z","shell.execute_reply.started":"2024-01-22T15:23:42.421204Z","shell.execute_reply":"2024-01-22T15:23:42.428199Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nResults for resnet34 architecture:\nImage 1: badger - 99.5%\nImage 2: wood rabbit - 89.4%\nImage 3: sundial - 100.0%\nImage 4: pineapple - 100.0%\nImage 5: table lamp - 38.9%\n\nResults for vgg16 architecture:\nImage 1: badger - 100.0%\nImage 2: wood rabbit - 98.9%\nImage 3: sundial - 100.0%\nImage 4: pineapple - 97.9%\nImage 5: vase - 15.9%\n\nResults for vgg19 architecture:\nImage 1: badger - 100.0%\nImage 2: wood rabbit - 97.3%\nImage 3: sundial - 100.0%\nImage 4: pineapple - 99.8%\nImage 5: ashcan - 13.0%\n\nResults for inceptionv3 architecture:\nImage 1: badger - 95.4%\nImage 2: wood rabbit - 90.6%\nImage 3: sundial - 85.4%\nImage 4: pineapple - 99.3%\nImage 5: table lamp - 17.2%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Tasks**\n1. Is there any significant difference between the results of different architectures?\n2. Try to classify several other images from the folders cnn_img/healthy and cnn_img/unhealthy that you choose on your own. Which cases are problematic?\n","metadata":{}},{"cell_type":"markdown","source":"For most images, **ResNet34** achieved high accuracy. VGG16 and VGG19 also performed well, but there are some differences in accuracy for specific images. InceptionV3 showed lower accuracy for some images compared to other architectures.","metadata":{}},{"cell_type":"code","source":"import os  \n\n# Assuming the folders are named \"healthy\" and \"unhealthy\" inside \"cnn_img\"\nhealthy_folder_path = \"../input/cnn-img/cnn_img/healthy\"\nunhealthy_folder_path = \"../input/cnn-img/cnn_img/unhealthy\"\n\n# Function to classify images and print predictions\ndef classify_images(folder_path, model):\n    image_paths = [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith(\".jpg\")]\n\n    for path in image_paths:\n        # Load and preprocess the image\n        img = read_image(path)\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n\n        # Make predictions\n        prediction = model(batch).squeeze(0).softmax(0)\n        class_id = prediction.argmax().item()\n        score = prediction[class_id].item()\n        category_name = weights.meta[\"categories\"][class_id]\n\n        # Print results\n        print(f\"Image: {path}\")\n        print(f\"Prediction: {category_name} ({100 * score:.1f}%)\")\n        print(\"-\" * 30)\n\n\n\n# Classify images in the 'healthy' folder\nprint(\"Classifying Healthy Images:\")\nclassify_images(healthy_folder_path, model)\n\nprint()\n# Classify images in the 'unhealthy' folder\nprint(\"Classifying Unhealthy Images:\")\nclassify_images(unhealthy_folder_path, model)","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:30:41.905377Z","iopub.execute_input":"2024-01-22T15:30:41.906207Z","iopub.status.idle":"2024-01-22T15:31:10.315887Z","shell.execute_reply.started":"2024-01-22T15:30:41.906173Z","shell.execute_reply":"2024-01-22T15:31:10.314957Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Classifying Healthy Images:\nImage: ../input/cnn-img/cnn_img/healthy/45.jpg\nPrediction: stinkhorn (14.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/56.jpg\nPrediction: Granny Smith (73.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/89.jpg\nPrediction: hamper (97.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/20.jpg\nPrediction: orange (83.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/58.jpg\nPrediction: eggnog (95.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/6.jpg\nPrediction: shopping basket (21.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/76.jpg\nPrediction: plate (32.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/71.jpg\nPrediction: hamper (48.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/5.jpg\nPrediction: fig (40.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/8.jpg\nPrediction: lemon (31.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/84.jpg\nPrediction: plate (63.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/85.jpg\nPrediction: grocery store (68.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/67.jpg\nPrediction: corn (28.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/82.jpg\nPrediction: strawberry (26.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/30.jpg\nPrediction: corn (40.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/97.jpg\nPrediction: broccoli (48.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/38.jpg\nPrediction: grocery store (21.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/42.jpg\nPrediction: cheeseburger (63.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/33.jpg\nPrediction: head cabbage (30.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/10.jpg\nPrediction: American lobster (42.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/54.jpg\nPrediction: head cabbage (21.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/0.jpg\nPrediction: corn (36.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/62.jpg\nPrediction: orange (52.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/35.jpg\nPrediction: hamper (26.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/61.jpg\nPrediction: strawberry (33.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/59.jpg\nPrediction: cauliflower (16.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/73.jpg\nPrediction: consomme (35.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/98.jpg\nPrediction: plate (39.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/41.jpg\nPrediction: hamper (18.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/94.jpg\nPrediction: bell pepper (52.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/60.jpg\nPrediction: hamper (48.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/57.jpg\nPrediction: bell pepper (75.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/91.jpg\nPrediction: fig (32.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/9.jpg\nPrediction: bell pepper (47.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/99.jpg\nPrediction: mixing bowl (35.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/37.jpg\nPrediction: hamper (20.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/1.jpg\nPrediction: plate (50.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/69.jpg\nPrediction: pizza (16.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/75.jpg\nPrediction: hot pot (28.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/81.jpg\nPrediction: strawberry (95.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/46.jpg\nPrediction: broccoli (85.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/44.jpg\nPrediction: strawberry (33.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/65.jpg\nPrediction: broccoli (40.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/50.jpg\nPrediction: Dutch oven (41.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/29.jpg\nPrediction: hot pot (77.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/79.jpg\nPrediction: plate (39.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/16.jpg\nPrediction: pineapple (84.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/55.jpg\nPrediction: Granny Smith (99.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/23.jpg\nPrediction: corn (9.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/7.jpg\nPrediction: cucumber (85.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/77.jpg\nPrediction: hen-of-the-woods (17.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/80.jpg\nPrediction: hot pot (62.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/28.jpg\nPrediction: trifle (57.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/22.jpg\nPrediction: trifle (37.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/40.jpg\nPrediction: butternut squash (28.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/48.jpg\nPrediction: hamper (65.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/24.jpg\nPrediction: shopping basket (79.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/88.jpg\nPrediction: hot pot (81.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/64.jpg\nPrediction: grocery store (13.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/31.jpg\nPrediction: hamper (84.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/43.jpg\nPrediction: hamper (98.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/13.jpg\nPrediction: pill bottle (64.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/74.jpg\nPrediction: custard apple (4.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/68.jpg\nPrediction: shopping basket (96.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/53.jpg\nPrediction: grocery store (58.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/83.jpg\nPrediction: hamper (22.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/72.jpg\nPrediction: guacamole (26.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/32.jpg\nPrediction: Dutch oven (95.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/17.jpg\nPrediction: hamper (74.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/26.jpg\nPrediction: hot pot (29.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/39.jpg\nPrediction: hamper (19.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/86.jpg\nPrediction: hamper (67.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/15.jpg\nPrediction: ice lolly (33.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/12.jpg\nPrediction: hamper (56.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/92.jpg\nPrediction: strawberry (82.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/11.jpg\nPrediction: cucumber (29.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/70.jpg\nPrediction: fig (45.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/34.jpg\nPrediction: hamper (98.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/27.jpg\nPrediction: corn (29.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/51.jpg\nPrediction: plate (56.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/52.jpg\nPrediction: custard apple (31.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/21.jpg\nPrediction: hot pot (77.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/4.jpg\nPrediction: ocarina (11.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/95.jpg\nPrediction: plate (16.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/3.jpg\nPrediction: broccoli (99.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/36.jpg\nPrediction: hotdog (75.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/96.jpg\nPrediction: hamper (80.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/63.jpg\nPrediction: grocery store (36.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/19.jpg\nPrediction: hamper (78.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/87.jpg\nPrediction: trifle (19.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/47.jpg\nPrediction: zucchini (49.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/93.jpg\nPrediction: cauliflower (47.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/14.jpg\nPrediction: cauliflower (28.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/18.jpg\nPrediction: hamper (84.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/78.jpg\nPrediction: lemon (29.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/49.jpg\nPrediction: hotdog (12.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/66.jpg\nPrediction: grocery store (27.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/2.jpg\nPrediction: hamper (29.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/90.jpg\nPrediction: grocery store (24.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/healthy/25.jpg\nPrediction: mixing bowl (13.2%)\n------------------------------\n\nClassifying Unhealthy Images:\nImage: ../input/cnn-img/cnn_img/unhealthy/45.jpg\nPrediction: cheeseburger (32.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/56.jpg\nPrediction: burrito (91.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/89.jpg\nPrediction: pizza (20.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/20.jpg\nPrediction: cheeseburger (98.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/58.jpg\nPrediction: cheeseburger (39.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/6.jpg\nPrediction: plate (29.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/76.jpg\nPrediction: pizza (83.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/71.jpg\nPrediction: cheeseburger (68.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/5.jpg\nPrediction: cheeseburger (94.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/8.jpg\nPrediction: hotdog (20.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/84.jpg\nPrediction: cheeseburger (53.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/85.jpg\nPrediction: cheeseburger (70.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/67.jpg\nPrediction: pizza (90.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/82.jpg\nPrediction: panpipe (17.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/30.jpg\nPrediction: French loaf (46.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/97.jpg\nPrediction: pizza (96.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/38.jpg\nPrediction: cheeseburger (73.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/42.jpg\nPrediction: cheeseburger (96.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/33.jpg\nPrediction: cheeseburger (93.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/10.jpg\nPrediction: hotdog (61.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/54.jpg\nPrediction: cheeseburger (90.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/0.jpg\nPrediction: cheeseburger (35.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/62.jpg\nPrediction: cheeseburger (45.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/35.jpg\nPrediction: cheeseburger (95.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/61.jpg\nPrediction: cheeseburger (69.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/59.jpg\nPrediction: cheeseburger (87.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/73.jpg\nPrediction: plate (40.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/98.jpg\nPrediction: hot pot (19.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/41.jpg\nPrediction: hotdog (75.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/94.jpg\nPrediction: burrito (51.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/60.jpg\nPrediction: pizza (86.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/57.jpg\nPrediction: carbonara (29.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/91.jpg\nPrediction: plate (53.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/9.jpg\nPrediction: cheeseburger (54.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/99.jpg\nPrediction: cheeseburger (95.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/37.jpg\nPrediction: cheeseburger (84.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/1.jpg\nPrediction: cheeseburger (97.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/69.jpg\nPrediction: plate (12.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/75.jpg\nPrediction: rotisserie (66.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/81.jpg\nPrediction: cheeseburger (99.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/46.jpg\nPrediction: plate (20.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/44.jpg\nPrediction: hot pot (20.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/65.jpg\nPrediction: cheeseburger (63.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/50.jpg\nPrediction: cheeseburger (42.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/29.jpg\nPrediction: burrito (49.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/79.jpg\nPrediction: cheeseburger (95.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/16.jpg\nPrediction: cheeseburger (80.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/55.jpg\nPrediction: pizza (72.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/23.jpg\nPrediction: cheeseburger (85.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/7.jpg\nPrediction: cheeseburger (51.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/77.jpg\nPrediction: hamper (37.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/80.jpg\nPrediction: cheeseburger (93.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/28.jpg\nPrediction: guacamole (56.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/22.jpg\nPrediction: plate (31.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/40.jpg\nPrediction: cheeseburger (97.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/48.jpg\nPrediction: cheeseburger (46.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/24.jpg\nPrediction: cheeseburger (34.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/88.jpg\nPrediction: pizza (84.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/64.jpg\nPrediction: burrito (56.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/31.jpg\nPrediction: plate (79.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/43.jpg\nPrediction: burrito (19.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/13.jpg\nPrediction: cheeseburger (96.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/74.jpg\nPrediction: cheeseburger (90.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/68.jpg\nPrediction: Dungeness crab (32.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/53.jpg\nPrediction: cheeseburger (53.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/83.jpg\nPrediction: cheeseburger (92.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/72.jpg\nPrediction: bagel (60.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/32.jpg\nPrediction: cheeseburger (78.7%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/17.jpg\nPrediction: burrito (97.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/26.jpg\nPrediction: plate (58.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/39.jpg\nPrediction: packet (18.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/86.jpg\nPrediction: pizza (80.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/15.jpg\nPrediction: plate (45.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/12.jpg\nPrediction: cheeseburger (96.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/92.jpg\nPrediction: fig (27.8%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/11.jpg\nPrediction: French loaf (39.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/70.jpg\nPrediction: bagel (78.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/34.jpg\nPrediction: cheeseburger (98.3%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/27.jpg\nPrediction: cheeseburger (98.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/51.jpg\nPrediction: plate (57.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/52.jpg\nPrediction: cheeseburger (98.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/21.jpg\nPrediction: pizza (94.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/4.jpg\nPrediction: corn (68.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/95.jpg\nPrediction: cheeseburger (98.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/3.jpg\nPrediction: cheeseburger (68.2%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/36.jpg\nPrediction: plate (28.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/96.jpg\nPrediction: cheeseburger (97.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/63.jpg\nPrediction: plate (28.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/19.jpg\nPrediction: meat loaf (51.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/87.jpg\nPrediction: pizza (88.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/47.jpg\nPrediction: plate (31.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/93.jpg\nPrediction: French loaf (53.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/14.jpg\nPrediction: cheeseburger (25.0%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/18.jpg\nPrediction: cheeseburger (55.4%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/78.jpg\nPrediction: carbonara (34.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/49.jpg\nPrediction: cheeseburger (97.6%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/66.jpg\nPrediction: wok (14.1%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/2.jpg\nPrediction: carbonara (29.5%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/90.jpg\nPrediction: plate (5.9%)\n------------------------------\nImage: ../input/cnn-img/cnn_img/unhealthy/25.jpg\nPrediction: cheeseburger (66.9%)\n------------------------------\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The problematic cases are those with multiple different objects, where the trained model provides a weaker score. Conversely, on images featuring the same object or multiple instances of the same object, the model tends to yield a higher score.","metadata":{}},{"cell_type":"markdown","source":"\n### 8.2.2 Creating your own classifier - pincers vs. scissors\nAlthough ImageNet has a lot of classes, sometimes they do not cover some desired cases. Let's assume that we want to tell images with pincers apart from the ones with scissors. Neither pincers nor scissors are among ImageNet classes. Nevertheless, we can still use some parts of the pre-trained models.\n\nVarious layers of a deep convolutional network have diferent tasks. The ones closest to the original input image usually look for features such as edges and corners i.e. for low-level features. After them there are layers that look for middle-level features such as circular objects, special curves, etc. Next, there are usually fully connected layers that create high-level semantic features by combining the information from the previous layers. These features are then used by the last layer that performs the actual classification. What we can do here is simply to discard the last layer i.e. not to calculate the class of an image, but to extract the values in on of the fully connected layers. This effectively means that we are going to use the network only as an extractor for high-level features that we would hardly be able to engineer on our own. Let's first see which layers can be found in the ResNet network:","metadata":{"id":"eZmSyBglaUoa"}},{"cell_type":"code","source":"import torchvision.models as models\nimport numpy as np\n\narchitecture=\"resnet34\"\n\nif architecture == \"resnet34\":\n  weights = models.ResNet34_Weights.DEFAULT\n  base_model = models.resnet34(weights=weights)\nelif architecture == \"resnet50\":\n  weights = models.ResNet50_Weights.DEFAULT\n  base_model = models.resnet50(weights=weights)\nelif architecture == \"vgg16\":\n  weights = models.VGG16_Weights.DEFAULT\n  base_model = models.vgg16(weights=weights)\n\nfor layer in base_model.children():\n    print(layer)","metadata":{"id":"ztcpyruTRTJD","execution":{"iopub.status.busy":"2024-01-22T15:46:19.726485Z","iopub.execute_input":"2024-01-22T15:46:19.727351Z","iopub.status.idle":"2024-01-22T15:46:20.082931Z","shell.execute_reply.started":"2024-01-22T15:46:19.727320Z","shell.execute_reply":"2024-01-22T15:46:20.081969Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\nBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nReLU(inplace=True)\nMaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (2): BasicBlock(\n    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (downsample): Sequential(\n      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (2): BasicBlock(\n    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (3): BasicBlock(\n    (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (downsample): Sequential(\n      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (2): BasicBlock(\n    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (3): BasicBlock(\n    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (4): BasicBlock(\n    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (5): BasicBlock(\n    (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\nSequential(\n  (0): BasicBlock(\n    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (downsample): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (1): BasicBlock(\n    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n  (2): BasicBlock(\n    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )\n)\nAdaptiveAvgPool2d(output_size=(1, 1))\nLinear(in_features=512, out_features=1000, bias=True)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"At the end you can see fully connected layer used for classification. We can extract the values from previous layers by using the following code:","metadata":{"id":"acvV9u06GocO"}},{"cell_type":"markdown","source":"These values can now be used as features and that can later be used with another classifier. Let's first extract the features for our pincer and scissors images.","metadata":{"id":"VjTMbaFPKfvk"}},{"cell_type":"code","source":"# Model without last fully connected layer\nmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n\nimg_path=\"../input/cnn-img/cnn_img/rabbit.jpg\"\n\nimg = read_image(img_path)\npreprocess = weights.transforms(antialias=True)\nbatch = preprocess(img).unsqueeze(0)\n\nfeatures = model(batch).squeeze(3).squeeze(2)\n\nprint(features.shape)\nfeature_layer_size=features.shape[1];","metadata":{"id":"Gxut2qMPJS3Y","execution":{"iopub.status.busy":"2024-01-22T15:48:25.750090Z","iopub.execute_input":"2024-01-22T15:48:25.750796Z","iopub.status.idle":"2024-01-22T15:48:25.852694Z","shell.execute_reply.started":"2024-01-22T15:48:25.750762Z","shell.execute_reply":"2024-01-22T15:48:25.851776Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"def create_numbered_paths(home_dir, n):\n    return [home_dir+str(i)+\".jpg\" for i in range(n)]\n\ndef create_paired_numbered_paths(first_home_dir, second_home_dir, n):\n    image_paths=[]\n    for p in zip(create_numbered_paths(first_home_dir, n), create_numbered_paths(second_home_dir, n)):\n        image_paths.extend(p)\n    return image_paths\n\ndef create_features(paths, verbose=True):\n    n=len(paths)\n    features=np.zeros((n, feature_layer_size))\n    for i in range(n):\n        if (verbose==True):\n            print(\"\\t%2d / %2d\"%(i+1, n))\n        img = read_image(paths[i])\n        preprocess = weights.transforms(antialias=True)\n        batch = preprocess(img).unsqueeze(0)\n        features[i, :]=model(batch).squeeze(3).squeeze(2).detach().numpy()\n\n    return features\n\npincers_dir=\"../input/cnn-img/cnn_img/pincers/\"\nscissors_dir=\"../input/cnn-img/cnn_img/scissors/\"\n\nindividual_n=50\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(pincers_dir, scissors_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the pincer image and 1 stands for the scissors image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=100\n#number of training images\nn_train=50\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n\n#train classes\ny_test=np.array(image_classes[n_train:])","metadata":{"id":"WPNycPJ4KhBQ","execution":{"iopub.status.busy":"2024-01-22T15:49:05.738364Z","iopub.execute_input":"2024-01-22T15:49:05.739294Z","iopub.status.idle":"2024-01-22T15:49:12.853681Z","shell.execute_reply.started":"2024-01-22T15:49:05.739258Z","shell.execute_reply":"2024-01-22T15:49:12.852558Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\nCreating test features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that for each image we have its features, we will divide the images into a training and a test set. Then we will use a linear SVM classifier to classify them.","metadata":{"id":"nQQp23pwweEf"}},{"cell_type":"code","source":"from sklearn import svm\n\ndef create_svm_classifier(x, y, C=1.0, kernel='linear'):\n    #we will use linear SVM\n    classifier=svm.SVC(kernel=kernel, C=C);\n    classifier.fit(x, y)\n    return classifier\n\ndef calculate_accuracy(classifier, x, y):\n    predicted=classifier.predict(x)\n    return np.sum(y==predicted)/y.size","metadata":{"id":"R12r9KSrgt95","execution":{"iopub.status.busy":"2024-01-22T15:49:29.043910Z","iopub.execute_input":"2024-01-22T15:49:29.044599Z","iopub.status.idle":"2024-01-22T15:49:29.585463Z","shell.execute_reply.started":"2024-01-22T15:49:29.044559Z","shell.execute_reply":"2024-01-22T15:49:29.584700Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"id":"ADOM91TWwdmS","execution":{"iopub.status.busy":"2024-01-22T15:49:40.004700Z","iopub.execute_input":"2024-01-22T15:49:40.005301Z","iopub.status.idle":"2024-01-22T15:49:40.019046Z","shell.execute_reply.started":"2024-01-22T15:49:40.005269Z","shell.execute_reply":"2024-01-22T15:49:40.018195Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Accuracy: 96.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Tasks**\n\n1. Is there any significant gain if more complex SVM models are used?\n2. What happens if we extract features using different backbone, e.g. vgg16?\n","metadata":{}},{"cell_type":"code","source":"C = [1e-2, 1, 1e2]\nkernel = ['poly','rbf','sigmoid']\n\nfor f in kernel:\n    for j in C:\n        classifier=create_svm_classifier(x_train, y_train, j, f)\n        #checking the model's accuracy\n        print(f\"Accuracy with hyperparameters {f.upper()}, {j}: {round(100*calculate_accuracy(classifier, x_test, y_test), 3)}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-22T15:58:29.711306Z","iopub.execute_input":"2024-01-22T15:58:29.711912Z","iopub.status.idle":"2024-01-22T15:58:29.740044Z","shell.execute_reply.started":"2024-01-22T15:58:29.711879Z","shell.execute_reply":"2024-01-22T15:58:29.739062Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Accuracy with hyperparameters POLY, 0.01: 96.0\nAccuracy with hyperparameters POLY, 1: 96.0\nAccuracy with hyperparameters POLY, 100.0: 96.0\nAccuracy with hyperparameters RBF, 0.01: 98.0\nAccuracy with hyperparameters RBF, 1: 98.0\nAccuracy with hyperparameters RBF, 100.0: 96.0\nAccuracy with hyperparameters SIGMOID, 0.01: 50.0\nAccuracy with hyperparameters SIGMOID, 1: 50.0\nAccuracy with hyperparameters SIGMOID, 100.0: 50.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It seems that there isn't a significant gain in accuracy when using more complex SVM models with different hyperparameters. The accuracy remains around 96-98% for the various combinations of kernel types (poly, rbf, sigmoid) and values of C (0.01, 1, 100.0). However, for the 'sigmoid' kernel, the accuracy is consistently 50.0%, which may indicate that this kernel is not suitable for my dataset. ","metadata":{}},{"cell_type":"code","source":"weights = models.VGG16_Weights.DEFAULT\nbase_model = models.vgg16(weights=weights)\n\n\nmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n\nimg_path=\"../input/cnn-img/cnn_img/rabbit.jpg\"\n\nimg = read_image(img_path)\npreprocess = weights.transforms(antialias=True)\nbatch = preprocess(img).unsqueeze(0)\n\nfeatures = model(batch).squeeze(3).squeeze(2)\n\nprint(features.shape)\nfeature_layer_size=features.shape[1];\n\n#number of all images\nn=100\n#number of training images\nn_train=50\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n\n#train classes\ny_test=np.array(image_classes[n_train:])\n\n#training the model\nclassifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n\nprint()\n#checking the model's accuracy\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:10:09.457630Z","iopub.execute_input":"2024-01-22T16:10:09.457990Z","iopub.status.idle":"2024-01-22T16:10:27.109301Z","shell.execute_reply.started":"2024-01-22T16:10:09.457963Z","shell.execute_reply":"2024-01-22T16:10:27.108377Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"torch.Size([1, 512])\nCreating training features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\nCreating test features...\n\t 1 / 50\n\t 2 / 50\n\t 3 / 50\n\t 4 / 50\n\t 5 / 50\n\t 6 / 50\n\t 7 / 50\n\t 8 / 50\n\t 9 / 50\n\t10 / 50\n\t11 / 50\n\t12 / 50\n\t13 / 50\n\t14 / 50\n\t15 / 50\n\t16 / 50\n\t17 / 50\n\t18 / 50\n\t19 / 50\n\t20 / 50\n\t21 / 50\n\t22 / 50\n\t23 / 50\n\t24 / 50\n\t25 / 50\n\t26 / 50\n\t27 / 50\n\t28 / 50\n\t29 / 50\n\t30 / 50\n\t31 / 50\n\t32 / 50\n\t33 / 50\n\t34 / 50\n\t35 / 50\n\t36 / 50\n\t37 / 50\n\t38 / 50\n\t39 / 50\n\t40 / 50\n\t41 / 50\n\t42 / 50\n\t43 / 50\n\t44 / 50\n\t45 / 50\n\t46 / 50\n\t47 / 50\n\t48 / 50\n\t49 / 50\n\t50 / 50\n\nAccuracy: 100.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It seems that when using **VGG16** as the backbone for feature extraction, my SVM model achieved a higher accuracy of 100% on the test set. On the other hand, when using **ResNet34** as the backbone, the accuracy was 96%.\n\nThis suggests that, at least for my task and dataset, VGG16 as a backbone for feature extraction might provide better discriminative features for  SVM classifier compared to ResNet34. The higher accuracy on the test set indicates that the features extracted using VGG16 were more effective in capturing the patterns","metadata":{}},{"cell_type":"markdown","source":"\n\n### 8.2.1 Creating your own classifier - healthy vs. unhealthy food\nThe previous example was relatively simple because all images were of same size and each of them had a white background, which allowed the extractor to concentrate only on the features of the actual objects. In this example we will use a slightly more complicated case - namely, will will tell images with healthy food apart from the ones with unhealthy food. FIrst let's repeat the same process as we did in the previous example and create the features:","metadata":{"id":"nSMhoT6swkeZ"}},{"cell_type":"code","source":"healthy_dir=\"../input/cnn-img/cnn_img/healthy/\"\nunhealthy_dir=\"../input/cnn-img/cnn_img/unhealthy/\"\n\nindividual_n=100\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the healthy image and 0 stands for the unhealthy image\n    image_classes.extend((0, 1))\n\n#number of all images\nn=200\n#number of training images\nn_train=100\n#number of test images\nn_test=n-n_train\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n#train classes\ny_test=np.array(image_classes[n_train:])","metadata":{"id":"JI75TDrVxNrR","execution":{"iopub.status.busy":"2024-01-22T16:12:46.653114Z","iopub.execute_input":"2024-01-22T16:12:46.653489Z","iopub.status.idle":"2024-01-22T16:13:19.033535Z","shell.execute_reply.started":"2024-01-22T16:12:46.653460Z","shell.execute_reply":"2024-01-22T16:13:19.032615Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nCreating test features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now let's train a model and test its accuracy:","metadata":{"id":"pfezje2qxQns"}},{"cell_type":"code","source":"classifier=create_svm_classifier(x_train, y_train, 1.0, \"linear\")\nprint(\"Accuracy: %.2lf%%\"%(100*calculate_accuracy(classifier, x_test, y_test)))","metadata":{"id":"TXUOBrkjxNzU","execution":{"iopub.status.busy":"2024-01-22T16:13:24.155298Z","iopub.execute_input":"2024-01-22T16:13:24.155683Z","iopub.status.idle":"2024-01-22T16:13:24.164462Z","shell.execute_reply.started":"2024-01-22T16:13:24.155651Z","shell.execute_reply":"2024-01-22T16:13:24.163612Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Accuracy: 92.00%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Tasks**\n1. Try the whole food classification with another network as a feature extractor and compare their results.\n2. What kind of test images are problematic?","metadata":{"id":"U-Zt5OzoxWqj"}},{"cell_type":"code","source":"# List of architectures\n\nhealthy_dir=\"../input/cnn-img/cnn_img/healthy/\"\nunhealthy_dir=\"../input/cnn-img/cnn_img/unhealthy/\"\n\nindividual_n=100\n#number of all images\nn=200\n#number of training images\nn_train=100\n#number of test images\nn_test=n-n_train\n\n# Results dictionary tostore accuracy for each architecture\n\n\n\narchitecture = \"resnet34\"\nweights = models.ResNet34_Weights.DEFAULT\nbase_model = models.resnet34(weights=weights)\n\n        \nmodel = torch.nn.Sequential(*(list(base_model.children())[:-1]), nn.AdaptiveAvgPool2d(1))\n\n#combining all image paths\nimage_paths=create_paired_numbered_paths(healthy_dir, unhealthy_dir, individual_n)\n\n#marking their classes\nimage_classes=[]\nfor i in range(individual_n):\n    #0 stands for the healthy image and 1 stands for the unhealthy image\n    image_classes.extend((0, 1))\n\nprint(\"Creating training features...\")\n#here we will store the features of training images\nx_train=create_features(image_paths[:n_train])\n#train classes\ny_train=np.array(image_classes[:n_train])\n\nprint(\"Creating test features...\")\n#here we will store the features of test images\nx_test=create_features(image_paths[n_train:])\n#train classes\ny_test=np.array(image_classes[n_train:])\n    \n# Training the model\nclassifier = create_svm_classifier(x_train, y_train, 1.0, \"linear\")\n\n# Checking the model's accuracy\naccuracy = calculate_accuracy(classifier, x_test, y_test)\nprint(f\"Accuracy: {round(100 * accuracy, 2)}%\\n\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-22T16:38:44.563410Z","iopub.execute_input":"2024-01-22T16:38:44.563807Z","iopub.status.idle":"2024-01-22T16:38:59.590049Z","shell.execute_reply.started":"2024-01-22T16:38:44.563774Z","shell.execute_reply":"2024-01-22T16:38:59.588964Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"Creating training features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nCreating test features...\n\t 1 / 100\n\t 2 / 100\n\t 3 / 100\n\t 4 / 100\n\t 5 / 100\n\t 6 / 100\n\t 7 / 100\n\t 8 / 100\n\t 9 / 100\n\t10 / 100\n\t11 / 100\n\t12 / 100\n\t13 / 100\n\t14 / 100\n\t15 / 100\n\t16 / 100\n\t17 / 100\n\t18 / 100\n\t19 / 100\n\t20 / 100\n\t21 / 100\n\t22 / 100\n\t23 / 100\n\t24 / 100\n\t25 / 100\n\t26 / 100\n\t27 / 100\n\t28 / 100\n\t29 / 100\n\t30 / 100\n\t31 / 100\n\t32 / 100\n\t33 / 100\n\t34 / 100\n\t35 / 100\n\t36 / 100\n\t37 / 100\n\t38 / 100\n\t39 / 100\n\t40 / 100\n\t41 / 100\n\t42 / 100\n\t43 / 100\n\t44 / 100\n\t45 / 100\n\t46 / 100\n\t47 / 100\n\t48 / 100\n\t49 / 100\n\t50 / 100\n\t51 / 100\n\t52 / 100\n\t53 / 100\n\t54 / 100\n\t55 / 100\n\t56 / 100\n\t57 / 100\n\t58 / 100\n\t59 / 100\n\t60 / 100\n\t61 / 100\n\t62 / 100\n\t63 / 100\n\t64 / 100\n\t65 / 100\n\t66 / 100\n\t67 / 100\n\t68 / 100\n\t69 / 100\n\t70 / 100\n\t71 / 100\n\t72 / 100\n\t73 / 100\n\t74 / 100\n\t75 / 100\n\t76 / 100\n\t77 / 100\n\t78 / 100\n\t79 / 100\n\t80 / 100\n\t81 / 100\n\t82 / 100\n\t83 / 100\n\t84 / 100\n\t85 / 100\n\t86 / 100\n\t87 / 100\n\t88 / 100\n\t89 / 100\n\t90 / 100\n\t91 / 100\n\t92 / 100\n\t93 / 100\n\t94 / 100\n\t95 / 100\n\t96 / 100\n\t97 / 100\n\t98 / 100\n\t99 / 100\n\t100 / 100\nAccuracy: 65.0%\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It appears that the **VGG16** architecture performs better in terms of accuracy (92.00%) compared to the **ResNet34** architecture (65.0%) for the task of classifying healthy and unhealthy food images. Therefore, the choice of the feature extractor architecture, in this case, has a significant impact on the classification performance.","metadata":{}},{"cell_type":"markdown","source":"Test images that are problematic include those with complex attributes, such as vibrant colors, intricate backgrounds, variations in lighting conditions, as well as low-resolution images and out-of-focus images. The presence of vibrant colors, intricate backgrounds, and varying lighting can contribute to difficulties in classification, adding complexity to the task. Additionally, low-resolution images and out-of-focus images pose challenges as they may lack the necessary details for the model to make accurate predictions. These diverse characteristics collectively make the classification task more challenging, impacting the model's ability to generalize effectively and resulting in potentially lower accuracy rates for such test cases.","metadata":{}}]}